# i2dl
Introduction to Deep Learning

## Stanford CS231n Notes

### 1. Neural Networks
- [Image Classification: Data-driven Approach, k-Nearest Neighbor, train/val/test splits.](https://github.com/zenilton-patrocinio/i2dl/blob/master/classification.md)<br/>
**Details:** *L1/L2 distances, hyperparameter search, cross-validation*

- Linear classification: Support Vector Machine, Softmax<br/>
**Details:** *parameteric approach, bias trick, hinge loss, cross-entropy loss, L2 regularization, web demo*

- Optimization: Stochastic Gradient Descent<br/>
**Details:** *optimization landscapes, local search, learning rate, analytic/numerical gradient*

- Backpropagation, Intuitions<br/>
**Details:** *chain rule interpretation, real-valued circuits, patterns in gradient flow*

- Neural Networks Part 1: Setting up the Architecture<br/>
**Details:** *model of a biological neuron, activation functions, neural net architecture, representational power*

- Neural Networks Part 2: Setting up the Data and the Loss<br/>
**Details:** *preprocessing, weight initialization, batch normalization, regularization (L2/dropout), loss functions*

- Neural Networks Part 3: Learning and Evaluation<br/>
**Details:** *gradient checks, sanity checks, babysitting the learning process, momentum (+nesterov), second-order methods, Adagrad/RMSprop, hyperparameter optimization, model ensembles*

- Putting it together: Minimal Neural Network Case Study<br/>
**Details:** *minimal 2D toy data example*

### 2. Convolutional Neural Networks
- Convolutional Neural Networks: Architectures, Convolution / Pooling Layers<br/>
**Details:** *layers, spatial arrangement, layer patterns, layer sizing patterns, AlexNet/ZFNet/VGGNet case studies, computational considerations*

- Understanding and Visualizing Convolutional Neural Networks<br/>
**Details:** *tSNE embeddings, deconvnets, data gradients, fooling ConvNets, human comparisons*

- Transfer Learning and Fine-tuning Convolutional Neural Networks
